\section{Conclusion}

\begin{frame}{Future Work}
	\begin{itemize}
		\item Explore more regularization methods to make energy difference more prominent
		\item Construct reward model from EBM
		\item Make model explainable by viewing $\|\nabla_\text{\mathbf{y}}{f_\theta(\mathbf{x},\mathbf{y})}\|$ for all pixels in $\mathbf{y}$
		\item Train a robot model with goal-conditioned reinforcement learning (GCRL) using EBM reward model
	\end{itemize}
\end{frame}

\begin{frame}{Conclusion}
	\begin{itemize}
		\item We need to make robot learning models \textbf{trustworthy}
		\item Aim to solve the problem of reward modeling in RL to learn possible superhuman policies
		\item Models that can capture the real world's complexity are key
		\item Regularized EBMs like JEPA better than contrastive approaches
	\end{itemize}
\end{frame}
