@article{black2024pi_0,
  title = {pi0: A Vision-Language-Action Flow Model for General Robot Control},
  author = {Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and
            Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy
            and Hausman, Karol and Ichter, Brian and others},
  journal = {arXiv preprint arXiv:2410.24164},
  year = {2024},
}

@article{intelligence2025pi_,
  title = {pi0.5: a Vision-Language-Action Model with Open-World Generalization},
  author = {Intelligence, Physical and Black, Kevin and Brown, Noah and
            Darpinian, James and Dhabalia, Karan and Driess, Danny and Esmail,
            Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and
            others},
  journal = {arXiv preprint arXiv:2504.16054},
  year = {2025},
}

@misc{openvla,
  title = {OpenVLA: An Open-Source Vision-Language-Action Model},
  author = {Moo Jin Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao
            and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan
            Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas
            Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and
            Sergey Levine and Percy Liang and Chelsea Finn},
  year = {2024},
  eprint = {2406.09246},
  archivePrefix = {arXiv},
  primaryClass = {cs.RO},
  url = {https://arxiv.org/abs/2406.09246},
}

@misc{act,
  title = {Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware},
  author = {Tony Z. Zhao and Vikash Kumar and Sergey Levine and Chelsea Finn},
  year = {2023},
  eprint = {2304.13705},
  archivePrefix = {arXiv},
  primaryClass = {cs.RO},
  url = {https://arxiv.org/abs/2304.13705},
}

@article{chi2024diffusionpolicy,
  author = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and
            Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song},
  title = {Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
  journal = {The International Journal of Robotics Research},
  year = {2024},
}

@article{Lipman2022FlowMF,
  title = {Flow Matching for Generative Modeling},
  author = {Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian
            Nickel and Matt Le},
  journal = {ArXiv},
  year = {2022},
  volume = {abs/2210.02747},
  url = {https://api.semanticscholar.org/CorpusID:252734897},
}

@article{Shukor2025SmolVLAAV,
  title = {SmolVLA: A Vision-Language-Action Model for Affordable and Efficient
           Robotics},
  author = {Mustafa Shukor and Dana Aubakirova and Francesco Capuano and Pepijn
            Kooijmans and Steven Palma and Adil Zouitine and Michel Aractingi and
            Caroline Pascal and Martino Russi and Andr{\'e}s Marafioti and Simon
            Alibert and Matthieu Cord and Thomas Wolf and R{\'e}mi Cad{\`e}ne},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2506.01844},
  url = {https://api.semanticscholar.org/CorpusID:279119427},
}

@article{Team2024OctoAO,
  title = {Octo: An Open-Source Generalist Robot Policy},
  author = {Octo Model Team and Dibya Ghosh and Homer Rich Walke and Karl
            Pertsch and Kevin Black and Oier Mees and Sudeep Dasari and Joey
            Hejna and Tobias Kreiman and Charles Xu and Jianlan Luo and You Liang
            Tan and Pannag R. Sanketi and Quan Vuong and Ted Xiao and Dorsa
            Sadigh and Chelsea Finn and Sergey Levine},
  journal = {ArXiv},
  year = {2024},
  volume = {abs/2405.12213},
  url = {https://api.semanticscholar.org/CorpusID:266379116},
}

@article{train-ebm,
  author = {Yang Song and Diederik P. Kingma},
  title = {How to Train Your Energy-Based Models},
  journal = {CoRR},
  volume = {abs/2101.03288},
  year = {2021},
  url = {https://arxiv.org/abs/2101.03288},
  eprinttype = {arXiv},
  eprint = {2101.03288},
  timestamp = {Thu, 21 Jan 2021 14:42:30 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-2101-03288.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}

@article{ebt,
  title = {Energy-Based Transformers are Scalable Learners and Thinkers},
  author = {Alexi Gladstone and Ganesh Nanduru and Md. Mofijul Islam and Peixuan
            Han and Hyeonjeong Ha and Aman Chadha and Yilun Du and Heng Ji and
            Jundong Li and Tariq Iqbal},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2507.02092},
  url = {https://api.semanticscholar.org/CorpusID:280148632},
}

@article{q-chunking,
  title = {Reinforcement Learning with Action Chunking},
  author = {Qiyang Li and Zhiyuan Zhou and Sergey Levine},
  journal = {ArXiv},
  year = {2025},
  volume = {abs/2507.07969},
  url = {https://api.semanticscholar.org/CorpusID:280283253},
}

@article{hil-serl,
  author = {Jianlan Luo and Charles Xu and Jeffrey Wu and Sergey Levine },
  title = {Precise and dexterous robotic manipulation via human-in-the-loop
           reinforcement learning},
  journal = {Science Robotics},
  volume = {10},
  number = {105},
  pages = {eads5033},
  year = {2025},
  doi = {10.1126/scirobotics.ads5033},
  URL = {https://www.science.org/doi/abs/10.1126/scirobotics.ads5033},
  eprint = {https://www.science.org/doi/pdf/10.1126/scirobotics.ads5033},
  abstract = {Robotic manipulation remains one of the most difficult challenges
              in robotics, with approaches ranging from classical model-based
              control to modern imitation learning. Although these methods have
              enabled substantial progress, they often require extensive manual
              design, struggle with performance, and demand large-scale data
              collection. These limitations hinder their real-world deployment at
              scale, where reliability, speed, and robustness are essential.
              Reinforcement learning (RL) offers a powerful alternative by
              enabling robots to autonomously acquire complex manipulation skills
              through interaction. However, realizing the full potential of RL in
              the real world remains challenging because of issues of sample
              efficiency and safety. We present a human-in-the-loop, vision-based
              RL system that achieved strong performance on a wide range of
              dexterous manipulation tasks, including precise assembly, dynamic
              manipulation, and dual-arm coordination. These tasks reflect
              realistic industrial tolerances, with small but critical variations
              in initial object placements that demand sophisticated reactive
              control. Our method integrates demonstrations, human corrections,
              sample-efficient RL algorithms, and system-level design to directly
              learn RL policies in the real world. Within 1 to 2.5 hours of
              real-world training, our approach outperformed other baselines by
              improving task success by 2×, achieving near-perfect success rates,
              and executing 1.8× faster on average. Through extensive experiments
              and analysis, our results suggest that RL can learn a wide range of
              complex vision-based manipulation policies directly in the real
              world within practical training times. We hope that this work will
              inspire a new generation of learned robotic manipulation techniques
              , benefiting both industrial applications and research
              advancements. A real-world reinforcement learning system achieved
              strong performance on challenging robotic manipulation tasks.},
}


@book{rl-book,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {
            https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis
            },
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 },
}

@article{Silver2016MasteringTG,
  title = {Mastering the game of Go with deep neural networks and tree search},
  author = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and
            L. Sifre and George van den Driessche and Julian Schrittwieser and
            Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and
            Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner
            and Ilya Sutskever and Timothy P. Lillicrap and Madeleine Leach and
            Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  journal = {Nature},
  year = {2016},
  volume = {529},
  pages = {484-489},
  url = {https://api.semanticscholar.org/CorpusID:515925},
}

@article{Dawid2023IntroductionTL,
  title = {Introduction to latent variable energy-based models: a path toward
           autonomous machine intelligence},
  author = {Anna Dawid and Yann LeCun},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  year = {2023},
  volume = {2024},
  url = {https://api.semanticscholar.org/CorpusID:259075148},
}

@misc{liu2022goalconditionedreinforcementlearningproblems,
  title = {Goal-Conditioned Reinforcement Learning: Problems and Solutions},
  author = {Minghuan Liu and Menghui Zhu and Weinan Zhang},
  year = {2022},
  eprint = {2201.08299},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  url = {https://arxiv.org/abs/2201.08299},
}
